{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a00daba5",
   "metadata": {},
   "source": [
    "## Car-Following trajectory analysis\n",
    "\n",
    "- Goal: Extract the acceleration histogram during car-following periods.\n",
    "\n",
    "- Sources of data:\n",
    "\n",
    "1. [The I-24 Motion dataset](https://i24motion.org/), [paper](https://i24motion.org/)\n",
    "    - Among all the different dates for which the data was made available, 4 hours of November-22, 2022 was used.\n",
    "    - The original sampling rate of this dataset is 0.04 second per timestep. This is linearly interpolated to obtain and standard 0.1 second per timestep.\n",
    "\n",
    "| Data Source                  | Duration | Length | Lanes | speed limit | Original Sampling rate | \n",
    "|------------------------------|------------------|----------------|-------|---------------| ---------------|\n",
    "| The I-24 Motion dataset     |   4 hours    |   6.75km  |   usually 4    |    70 mph  |  0.04   |\n",
    "\n",
    "----\n",
    "\n",
    "Car following filter used to extract car-following periods:\n",
    "- Vehicle types: \n",
    "    - For I-24: vehicle class 0: sedan, 1: midsize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95549d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ijson \n",
    "import bigjson\n",
    "import scipy.io \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "datapath = './Data/i_24/637c399add50d54aa5af0cf4__post2.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf9ac27",
   "metadata": {},
   "source": [
    "## First: I-24 Motion dataset\n",
    "Notes: \n",
    "- Data covers both directions with multiple lanes\n",
    "- \n",
    "\n",
    "<img width=\"900\" alt=\"1\" src=\"https://github.com/poudel-bibek/AI-Assignments/assets/96804013/2ef05f8f-c2c7-4350-91dc-fdd5c30fd705\">\n",
    "\n",
    "Total: 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd7861c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(datapath, 'rb') as file:\n",
    "    json_data = bigjson.load(file)\n",
    "    \n",
    "    datum = json_data[10]\n",
    "\n",
    "    # What are the various keys in data\n",
    "    keys = datum.keys()\n",
    "    print(f\"Total: {len(keys)}\\n\")\n",
    "    for i in range(len(keys)):\n",
    "        print(f\"{i+1}: {keys[i-1]}\")\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "    for key, value in datum.iteritems():\n",
    "        print(key, value)\n",
    "\n",
    "file.close()\n",
    "\n",
    "# Extra 9\n",
    "# 'coarse_vehicle_class', 'fine_vehicle_class', 'x_score', 'y_score', 'compute_node_id', 'local_fragment_id', 'merged_ids', 'road_segment_ids', 'flags'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b5e5df",
   "metadata": {},
   "source": [
    "### Step 1: Resample + Save only the required data to CSV\n",
    "- Linear interpolation seems difficult:\n",
    "    - Computationally expensive (crashes the notebook)  \n",
    "    - Time consuming compared to resampling\n",
    "    - Cant get precise 0.1 intervals (shocking), np.arange can only generate 0.1, 0.\n",
    "    - Resampling only has a small error at every other data point by 0.02 seconds (first data point is 0.0, then 0.12, then 0.20, then 0.32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010faae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 3: Process data in chunks\n",
    "chunk_size = 10000000  # 10 million\n",
    "columns=['id', 'timestamp', 'x_position', 'y_position', 'class_fine', 'class_coarse', 'direction']\n",
    "\n",
    "def process_datum(datum):\n",
    "    data_points = []\n",
    "    id = datum[\"_id\"]['$oid']\n",
    "    timestamp = datum[\"timestamp\"]\n",
    "    x_position = datum[\"x_position\"]\n",
    "    y_position = datum[\"y_position\"]\n",
    "    class_fine = datum[\"fine_vehicle_class\"]\n",
    "    class_coarse = datum[\"coarse_vehicle_class\"]\n",
    "    direction = datum[\"direction\"]\n",
    "    \n",
    "    length = len(timestamp)\n",
    "    for j in range(length):\n",
    "        data_point = [id, timestamp[j], x_position[j], y_position[j], class_fine, class_coarse, direction]\n",
    "        data_points.append(data_point)\n",
    "        \n",
    "    return data_points\n",
    "\n",
    "# Open the JSON file\n",
    "with open(datapath, 'r') as file:\n",
    "    # Parse the JSON objects one by one\n",
    "    parser = ijson.items(file, 'item')\n",
    "    chunks = []\n",
    "    \n",
    "    # start by adding a header \n",
    "    temp_df = pd.DataFrame(columns=columns)\n",
    "    temp_df.to_csv('temporary_chunks.csv', mode='a', header=True, index=False)\n",
    "    \n",
    "    try:\n",
    "        i = 1\n",
    "        while True:\n",
    "            datum = next(parser)\n",
    "            chunks.extend(process_datum(datum))\n",
    "            \n",
    "            if len(chunks) >= chunk_size:\n",
    "                temp_df = pd.DataFrame(chunks, columns=columns)\n",
    "                temp_df.to_csv('temporary_chunks.csv', mode='a', header=False, index=False)  # Saving chunks to a temporary file\n",
    "                chunks = []\n",
    "                print(f\"Chunk {i}, {i*10}M data processed.\")\n",
    "                i += 1\n",
    "                \n",
    "    except StopIteration:\n",
    "        print(\"All data has been processed.\")\n",
    "        \n",
    "    finally:\n",
    "        if chunks:  # Saving remaining data\n",
    "            temp_df = pd.DataFrame(chunks, columns=columns)\n",
    "            temp_df.to_csv('temporary_chunks.csv', mode='a', header=False, index=False)\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6506e7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('temporary_chunks.csv')\n",
    "\n",
    "# Converting 'timestamp' to numeric values, setting errors='coerce' to handle non-numeric values\n",
    "df['float_timestamp'] = pd.to_numeric(df['timestamp'], errors='coerce')\n",
    "\n",
    "# Dropping the original 'timestamp' column\n",
    "df = df.drop(['timestamp'], axis=1)\n",
    "\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c26d24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of data, what values can variables take\n",
    "print(f\"Unique ids (Before): {len(df['id'].unique())}\\n\")\n",
    "print(f\"Fine: {df['class_fine'].value_counts()}\\n\")\n",
    "print(f\"Coarse classes (Before):\\n{df['class_coarse'].value_counts()}\")\n",
    "\n",
    "# Fine class can be dropped (no use)\n",
    "df = df.drop(['class_fine'], axis=1)\n",
    "\n",
    "# Filter data by class only keep class 0 and 1\n",
    "df = df[df['class_coarse'].isin([0, 1])]\n",
    "print(f\"Unique ids (After): {len(df['id'].unique())}\\n\")\n",
    "print(f\"\\nCoarse After (After):\\n{df['class_coarse'].value_counts()}\")\n",
    "\n",
    "# Corse class can also be dropped (made use of it)\n",
    "df = df.drop(['class_coarse'], axis=1)\n",
    "\n",
    "print(f\"Shape of new data: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac8b069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample\n",
    "new_df = pd.DataFrame(columns=['id', 't', 'x', 'y', 'direction'])\n",
    "\n",
    "# Create a list to temporarily store the rows\n",
    "rows_to_append = []\n",
    "\n",
    "# Number of rows to process before printing a progress update\n",
    "progress_interval = 10000000 # 10 million\n",
    "\n",
    "# Iterating over DataFrame rows as tuples\n",
    "for i, row in enumerate(df.itertuples(index=False)):\n",
    "    \n",
    "    if i % progress_interval == 0:\n",
    "        print(f\"Processed {i} rows so far...\")\n",
    "    \n",
    "    # Check if rows_to_append is not empty and if the id is the same as the previous one\n",
    "    if rows_to_append and row.id == rows_to_append[-1]['id']:\n",
    "        if str(row.float_timestamp)[11] != str(rows_to_append[-1]['t'])[11]:\n",
    "            rows_to_append.append({'id': row.id, 't': row.float_timestamp, 'x': row.x_position, 'y': row.y_position, 'direction': row.direction})\n",
    "    else:\n",
    "        rows_to_append.append({'id': row.id, 't': row.float_timestamp, 'x': row.x_position, 'y': row.y_position, 'direction': row.direction})\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "new_df = pd.DataFrame(rows_to_append)\n",
    "\n",
    "print(f\"Processing complete. {len(new_df)} rows added.\")\n",
    "new_df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c37764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv file\n",
    "new_df.to_csv('./Data/i_24/resampled_i24.csv', header=True, index=False)\n",
    "# From the initial 247 million data points, reduced to 74 million."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7792829",
   "metadata": {},
   "source": [
    "### Step 2: Remove extra lanes and assign lane ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab73aacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_df = pd.read_csv('./Data/i_24/resampled_i24.csv')\n",
    "loaded_df.head(20)\n",
    "\n",
    "subset_df = loaded_df #.iloc[:1000000].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291441eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "directions = subset_df['direction'].unique()\n",
    "direction_mapping = {-1: 'Westbound', 1: 'Eastbound'}\n",
    "\n",
    "plt.figure(figsize=(15, 8))  # Adjusted size for two subplots\n",
    "\n",
    "lane_definitions = {}\n",
    "\n",
    "for index, direction in enumerate(directions):\n",
    "    plt.subplot(1, 2, index + 1)  # 1 row, 2 columns, subplot index\n",
    "\n",
    "    direction_df = subset_df[subset_df['direction'] == direction].copy()\n",
    "    num_clusters = 5\n",
    "\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    direction_df['cluster'] = kmeans.fit_predict(direction_df[['y']])\n",
    "\n",
    "    for cluster_id in direction_df['cluster'].unique():\n",
    "        cluster_data = direction_df[direction_df['cluster'] == cluster_id]\n",
    "        plt.scatter(cluster_data['x'], cluster_data['y'], label=f'Cluster {cluster_id}', alpha=0.5)\n",
    "\n",
    "    sorted_centroids = sorted(kmeans.cluster_centers_[:, 0])\n",
    "    lane_boundaries = []\n",
    "    for i in range(len(sorted_centroids) - 1):\n",
    "        boundary = (sorted_centroids[i] + sorted_centroids[i + 1]) / 2\n",
    "        lane_boundaries.append(boundary)\n",
    "\n",
    "    lane_definitions[direction_mapping[direction]] = lane_boundaries\n",
    "\n",
    "    # Calculate the y-tick locations\n",
    "    y_min, y_max = plt.ylim()\n",
    "    y_interval = (y_max - y_min) / 24\n",
    "    y_ticks = [y_min + i * y_interval for i in range(25)]\n",
    "    plt.yticks(y_ticks)\n",
    "\n",
    "    plt.xlabel('X (Longitudinal)')\n",
    "    plt.ylabel('Y (Lateral)')\n",
    "    plt.title(f'{direction_mapping[direction]}')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Clustering of Y-positions to identify lanes within directions', y=1.05) \n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Preliminary Lane ranges\n",
    "# print(\"Premliminary Lane Ranges\\n\")\n",
    "# for direction, boundaries in lane_definitions.items():\n",
    "#     print(f\"{direction} Lane Boundaries:\")\n",
    "#     for i, boundary in enumerate(boundaries):\n",
    "#         if i == 0:\n",
    "#             size = boundary - y_min\n",
    "#             print(f\"Lane 1: < {boundary} (Size: {size:.2f})\")\n",
    "#         else:\n",
    "#             size = boundary - boundaries[i-1]\n",
    "#             print(f\"Lane {i+1}: {boundaries[i-1]} to {boundary} (Size: {size:.2f})\")\n",
    "    \n",
    "#     size = y_max - boundaries[-1]\n",
    "#     print(f\"Lane {len(boundaries)+1}: > {boundaries[-1]} (Size: {size:.2f})\")\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c9a9ac",
   "metadata": {},
   "source": [
    "<img width=\"700\" alt=\"poles_lanes\" src=\"https://github.com/poudel-bibek/AI-Assignments/assets/96804013/71b81420-6dbe-46f8-a2aa-d599241913c1\">\n",
    "<p>\n",
    "From the real-world: Its mostly always 4 lane\n",
    "\n",
    "Remove the entire vehicle if:\n",
    "- For any vehicle Westbound (-1), if a trajectory has a value lower than -60 or higher than -8\n",
    "- For any vehicle Eastbound (1), if a trajectory has a value lower than  2 or higher than 58\n",
    "\n",
    "__further revise the lane boundaries as follows__\n",
    "__The typical size of a lane in interstate highways is 12 ft, use the Figure below from the paper as well__\n",
    "\n",
    "<img width=\"700\" alt=\"paper_lanes\" src=\"https://github.com/poudel-bibek/4-Degrees-Of-Freedom/assets/96804013/51bba06b-acef-4ffb-9198-3044e469f425\n",
    "\">\n",
    "\n",
    "This figure in the paper is misleading/ confusing (It does say that its just of a couple sections). I am not following the figure for lane demarcations, following what is shown below.\n",
    "\n",
    "| Lane | Westbound      | Eastbound        |\n",
    "|------|----------------|------------------|\n",
    "| 1    |   3 to 15      | -9 to -21        |\n",
    "| 2    |   16 to 28     | -22 to -34       |\n",
    "| 3    |   29 to 41     | -35 to -47       |\n",
    "| 4    |   42 to 54    | -48 to -60       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6764b771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the thresholds\n",
    "upper_threshold_east = -8 #54\n",
    "lower_threshold_east = -60 # 2\n",
    "\n",
    "upper_threshold_west = 54 #-8\n",
    "lower_threshold_west = 2 #-64\n",
    "\n",
    "# Remove vehicles that don't meet the criteria\n",
    "# This will also remove the vehicles that at any point exit the highway\n",
    "to_remove_east = subset_df[\n",
    "    (subset_df['direction'] == 1) & # Eastbound \n",
    "    ((subset_df['y'] > upper_threshold_east) | (subset_df['y'] < lower_threshold_east)) # Data points to remove\n",
    "]['id'].unique()\n",
    "\n",
    "to_remove_west = subset_df[\n",
    "    (subset_df['direction'] == -1) & # Westbound\n",
    "    ((subset_df['y'] > upper_threshold_west) | (subset_df['y'] < lower_threshold_west)) # Data points to remove\n",
    "]['id'].unique()\n",
    "\n",
    "subset_df = subset_df[~subset_df['id'].isin(to_remove_east)]\n",
    "subset_df = subset_df[~subset_df['id'].isin(to_remove_west)]\n",
    "\n",
    "print(f\"Shape of the remaining data: {subset_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71492d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redo the clustering plot for validation\n",
    "directions = subset_df['direction'].unique()\n",
    "direction_mapping = {-1: 'Westbound', 1: 'Eastbound'}\n",
    "\n",
    "plt.figure(figsize=(15, 10))  # Adjusted size for two subplots\n",
    "\n",
    "lane_definitions = {}\n",
    "\n",
    "for index, direction in enumerate(directions):\n",
    "    plt.subplot(1, 2, index + 1)  # 1 row, 2 columns, subplot index\n",
    "\n",
    "    direction_df = subset_df[subset_df['direction'] == direction].copy()\n",
    "    num_clusters = 5\n",
    "\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    direction_df['cluster'] = kmeans.fit_predict(direction_df[['y']])\n",
    "\n",
    "    for cluster_id in direction_df['cluster'].unique():\n",
    "        cluster_data = direction_df[direction_df['cluster'] == cluster_id]\n",
    "        plt.scatter(cluster_data['x'], cluster_data['y'], label=f'Cluster {cluster_id}', alpha=0.5)\n",
    "\n",
    "    sorted_centroids = sorted(kmeans.cluster_centers_[:, 0])\n",
    "    lane_boundaries = []\n",
    "    for i in range(len(sorted_centroids) - 1):\n",
    "        boundary = (sorted_centroids[i] + sorted_centroids[i + 1]) / 2\n",
    "        lane_boundaries.append(boundary)\n",
    "\n",
    "    lane_definitions[direction_mapping[direction]] = lane_boundaries\n",
    "\n",
    "    # Calculate the y-tick locations\n",
    "    y_min, y_max = plt.ylim()\n",
    "    y_interval = (y_max - y_min) / 24\n",
    "    y_ticks = [y_min + i * y_interval for i in range(25)]\n",
    "    plt.yticks(y_ticks)\n",
    "\n",
    "    plt.xlabel('X (Longitudinal)')\n",
    "    plt.ylabel('Y (Lateral)')\n",
    "    plt.title(f'{direction_mapping[direction]}')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Clustering of Y-positions to identify lanes within directions', y=1.05) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50380c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_lane_vectorized(y_values, directions):\n",
    "    # Define boundaries. These dont match the table (but to make the output match, because of how left, right work below they have to be set this way)\n",
    "    eastbound_boundaries = np.array([-8, -21, -34, -47, -60])  # 1\n",
    "    westbound_boundaries = np.array([2, 16, 29, 42, 55])       # -1\n",
    "\n",
    "    # Prepare output array\n",
    "    lanes = np.zeros_like(y_values, dtype=np.int)\n",
    "\n",
    "    # For Eastbound: directions == 1\n",
    "    # Invert the eastbound_boundaries to ascending order for searchsorted\n",
    "    ascending_boundaries_east = eastbound_boundaries[::-1]\n",
    "    eastbound_indices = np.searchsorted(ascending_boundaries_east, y_values, side='right')\n",
    "    eastbound_lanes = len(ascending_boundaries_east) - eastbound_indices\n",
    "\n",
    "    # For Westbound: directions == -1\n",
    "    westbound_lanes = np.searchsorted(westbound_boundaries, y_values, side='left')\n",
    "\n",
    "    # Assign lanes based on direction\n",
    "    lanes[directions == 1] = eastbound_lanes[directions == 1]\n",
    "    lanes[directions == -1] = westbound_lanes[directions == -1]\n",
    "\n",
    "    # There should be no values outside the range\n",
    "    # Handle values outside the boundaries for eastbound\n",
    "    #lanes[(directions == 1) & (y_values > -8)] = 0\n",
    "    #lanes[(directions == 1) & (y_values < -60)] = 5\n",
    "\n",
    "    # Handle values outside the boundaries for westbound\n",
    "    #lanes[(directions == -1) & (y_values < 2)] = 0\n",
    "    #lanes[(directions == -1) & (y_values > 54)] = 5\n",
    "\n",
    "    return lanes\n",
    "\n",
    "# Apply the function and assign the lane IDs\n",
    "subset_df['lane_id'] = assign_lane_vectorized(subset_df['y'].values, subset_df['direction'].values)\n",
    "\n",
    "# Save this to a CSV file\n",
    "subset_df.to_csv('./Data/i_24/processed_i24.csv', header=True, index=False)\n",
    "\n",
    "# Display the first few rows\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "print(subset_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a1a24e",
   "metadata": {},
   "source": [
    "### Step 3: Find leader, get leader params: id, headway\n",
    "- A leader only exists if the ego and leader are in the same lane i.e., in the same timesteps, leader is going in same direction + lane as ego vehicle with difference in x\n",
    "- The difference in x_position is the headway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a11c930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "processed_df = pd.read_csv('./Data/i_24/processed_i24.csv')\n",
    "\n",
    "# How are lanes distributed?\n",
    "# print(processed_df['lane_id'].value_counts())\n",
    "\n",
    "# # Check for Nan or missing values\n",
    "# print(processed_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2512eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find leader and add the id and headway, if no leader then -1 \n",
    "# Around the same time stamp, the leader travels the same lane as ego vehicle with a difference in x position\n",
    "# Only cosidered a leader if the difference in x position is positive 150m\n",
    "# Eastbound (1) or Westbound (-1), x-values are always positive. \n",
    "# In Westbound, the leader will have a x-value higher than follower\n",
    "# In Eastbound, the leader will have a x-value lower than follower\n",
    "\n",
    "# What if there are multiple leaders?\n",
    "# For now, we will only consider the closest leader. But how to get the closest leader?\n",
    "# We can sort the leader dataframe by x-value and take the first row.\n",
    "\n",
    "def find_leader(group, threshold):\n",
    "\n",
    "    # Initialize the leader DataFrame with the same index as group\n",
    "    leader_df = pd.DataFrame(index=group.index)\n",
    "    leader_df['leader'] = -1\n",
    "    leader_df['headway'] = -1\n",
    "\n",
    "    # Iterate over the unique IDs directly\n",
    "    for unique_id in group['id'].unique():\n",
    "        current_vehicle = group[group['id'] == unique_id]\n",
    "        x_value = current_vehicle['x'].iloc[0]\n",
    "        direction = current_vehicle['direction'].iloc[0]\n",
    "\n",
    "        # Define masks for potential leaders based on direction\n",
    "        mask_eastbound = (group['x'] < x_value) & (group['direction'] == 1)\n",
    "        mask_westbound = (group['x'] > x_value) & (group['direction'] == -1)\n",
    "        mask_potential_leaders = mask_eastbound if direction == 1 else mask_westbound\n",
    "\n",
    "        # Find potential leaders\n",
    "        potential_leaders = group[mask_potential_leaders].copy()\n",
    "        if not potential_leaders.empty:\n",
    "            potential_leaders['x_diff'] = (potential_leaders['x'] - x_value).abs()\n",
    "            leader = potential_leaders[potential_leaders['x_diff'] < threshold].nsmallest(1, 'x_diff')\n",
    "\n",
    "            if not leader.empty:\n",
    "                leader_index = group[group['id'] == unique_id].index\n",
    "                leader_df.loc[leader_index, 'leader'] = leader['id'].values[0]\n",
    "                leader_df.loc[leader_index, 'headway'] = leader['x_diff'].values[0]\n",
    "\n",
    "    # Concatenate the group data with the leader data\n",
    "    result = pd.concat([group.reset_index(drop=True), leader_df.reset_index(drop=True)], axis=1)\n",
    "    return result\n",
    "\n",
    "# Open CSV file for appending results\n",
    "csv_file_path = Path('./Data/i_24/second_processed_i24.csv')\n",
    "csv_exists = csv_file_path.exists()\n",
    "\n",
    "groups = processed_df.groupby(['t', 'lane_id', 'direction'])\n",
    "\n",
    "# Pre-calculate the number of groups to avoid recomputing it every iteration\n",
    "print(\"Total groups:\", len(groups))\n",
    "\n",
    "# Process the dataframe in chunks\n",
    "group_count = 0\n",
    "\n",
    "# Because we have not converted to meters yet\n",
    "threshold =  492.126 # This is 150meters in ft # Max headway to be considered a leader\n",
    "results = []  # Initialize a list to store intermediate dataframes\n",
    "\n",
    "for name, group in groups:\n",
    "    group_count += 1 \n",
    "    results.append(find_leader(group, threshold))\n",
    "\n",
    "    # Check if we've processed 20000 groups, or if we are at the last one. \n",
    "    if group_count % 20000 == 0:\n",
    "\n",
    "        print(f\"Processing chunk at group #{group_count}: t={group['t'].iloc[0]}\")\n",
    "\n",
    "        # Concatenate all the dataframes in the list\n",
    "        result_df = pd.concat(results, ignore_index=True)\n",
    "        \n",
    "        # Write the result_df to CSV file in append mode\n",
    "        result_df.to_csv(csv_file_path, mode='a', header=not csv_exists, index=False)\n",
    "        \n",
    "        csv_exists = True\n",
    "\n",
    "        # Clear the list for the next chunk\n",
    "        results = []\n",
    "\n",
    "# If there are any remaining groups that haven't been written after the loop\n",
    "if results:\n",
    "    result_df = pd.concat(results, ignore_index=True)\n",
    "    with open(csv_file_path, 'a') as f:\n",
    "        result_df.to_csv(f, header=not csv_file_path.exists(), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee4e434",
   "metadata": {},
   "source": [
    "### Step 4: Go through all vehicles and calculate thier instantaneous Velocity and Acceleration.\n",
    "First perform a number of smaller steps\n",
    "- Y values are not needed drop them\n",
    "- Convert x values to meters \n",
    "- Filter: Only consider a vehicle if it has a leader.\n",
    "\n",
    "Then:\n",
    "- Only if the difference in two consecitive timesteps is either 0.8 or 0.12 ~ both cases approximated as 0.1\n",
    "- Needs initial two values to be removed (because of differentiation twice) for every data point (to avoid having Nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01490a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load last_processed_i24\n",
    "new_df = pd.read_csv('./Data/i_24/second_processed_i24.csv')\n",
    "\n",
    "# drop y_values\n",
    "new_df = new_df.drop(['y'], axis=1)\n",
    "\n",
    "# Convert distances in ft to meters\n",
    "new_df['x'] = new_df['x'] * 0.3048\n",
    "new_df['headway'] = new_df['headway'] * 0.3048\n",
    "\n",
    "print(f\"Shape of the new data: {new_df.shape}\")\n",
    "print(new_df.dtypes)\n",
    "\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "new_df.head(20)\n",
    "# 74 Million"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b836a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only consider data points with a leader\n",
    "with_leader_df = new_df[new_df['leader'] != '-1'] # for some reason -1 has to be a string.\n",
    "\n",
    "print(f\"Shape of the new data: {with_leader_df.shape}\")\n",
    "print(with_leader_df.dtypes)\n",
    "\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "with_leader_df.head(20)\n",
    "# 30 Million"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4afa2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate velocity and acceleration of the current vehicle\n",
    "# For every vehicle id, if the difference between two consecutive timesteps is either 0.12 or 0.08, then we can calculate the velocity and acceleration\n",
    "# remove the two initial points?\n",
    "\n",
    "def calculate_velocity_acceleration(df):\n",
    "    # sort the data, need only t because grouped by id already\n",
    "    df = df.sort_values(['t'])  \n",
    "\n",
    "    # Calculate the differences\n",
    "    df['delta_x'] = df.groupby('id')['x'].diff()\n",
    "    df['delta_t'] = df.groupby('id')['t'].diff() # Divide by the actual time-step 0.08 or 0.12 # To avoid approximation error\n",
    "\n",
    "    # Avoid division by zero by replacing zero in delta_t with NaN\n",
    "    df['delta_t'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "    # Calculate velocity and acceleration\n",
    "    df['velocity'] = df['delta_x'] / df['delta_t']\n",
    "    df['acceleration'] = df.groupby('id')['velocity'].diff() / df['delta_t']\n",
    "\n",
    "    # Drop the intermediate delta columns if they are not needed\n",
    "    df.drop(['delta_x', 'delta_t'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "# Path to CSV file\n",
    "csv_file_path = './Data/i_24/velocity_acceleration_i24.csv'\n",
    "csv_exists = Path(csv_file_path).exists()\n",
    "\n",
    "# Group the dataframe by the required fields\n",
    "groups = with_leader_df.groupby(['id'])  \n",
    "print(\"Found\", len(groups), \"groups.\")\n",
    "\n",
    "results = []  # Initialize a list to store intermediate dataframes\n",
    "for group_idx, (group_name, group) in enumerate(groups, start=1):\n",
    "    results.append(calculate_velocity_acceleration(group))\n",
    "    \n",
    "    # Check if we've processed a chunk's worth of groups or if we are at the last one.\n",
    "    if group_idx % 20000 == 0:\n",
    "        print(f\"Processing chunk at group #{group_idx}\")\n",
    "        \n",
    "        # Concatenate all the dataframes in the list\n",
    "        result_df = pd.concat(results, ignore_index=True)\n",
    "        \n",
    "        # Write the result_df to CSV file in append mode\n",
    "        result_df.to_csv(csv_file_path, mode='a', header=not csv_exists, index=False)\n",
    "        csv_exists = True\n",
    "        \n",
    "        # Clear the list for the next chunk\n",
    "        results = []\n",
    "\n",
    "# If there are any remaining groups that haven't been written after the loop\n",
    "if results:\n",
    "    result_df = pd.concat(results, ignore_index=True)\n",
    "    result_df.to_csv(csv_file_path, header=not csv_exists, index=False)\n",
    "\n",
    "# 360,000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8a5823",
   "metadata": {},
   "source": [
    "### Step 5: Apply remaining of the car-following filter (in the given order)\n",
    "\n",
    "- Follower speed is greater than 10% of the speed limit (in m/s) i.e., 3.12 m/s (to avoid approach to standing traffic).\n",
    "- Space headway is less than 62m. Apply 2 second rule to the speed limit (avoid free-flow).\n",
    "- Both leader and follower remain in the same lane for > 10s\n",
    "\n",
    "__Note: longitudional data is already in meters, meters per second, meters per second squared__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d105969d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data \n",
    "velocity_accn_df = pd.read_csv('./Data/i_24/velocity_acceleration_i24.csv')\n",
    "# add header \n",
    "velocity_accn_df.columns = ['direction', 'id', 't', 'x', 'lane_id', 'leader_id', 'headway', 'velocity', 'acceleration']\n",
    "\n",
    "# size of data\n",
    "print(f\"Shape of the new data: {velocity_accn_df.shape}\")\n",
    "\n",
    "# remove all rows with NaN\n",
    "velocity_accn_df = velocity_accn_df.dropna()\n",
    "\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "velocity_accn_df.head(20)\n",
    "# 275,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446b969c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of velocity and headways \n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax[0].hist(velocity_accn_df['velocity'], bins=250, color='blue')\n",
    "ax[0].set_xlabel('Velocity (m/s)')\n",
    "ax[0].set_ylabel('Frequency')\n",
    "ax[0].set_title('Velocity Distribution')\n",
    "\n",
    "ax[1].hist(velocity_accn_df['headway'], bins=250, color='green')\n",
    "ax[1].set_xlabel('Headway (m)')\n",
    "ax[1].set_ylabel('Frequency')\n",
    "ax[1].set_title('Headway Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80cc678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with velocity less than 3.125 m/s and higher than -3.125 m/s i.e. only consider outside this range\n",
    "filtered_accn_df = velocity_accn_df[~velocity_accn_df['velocity'].between(-3.125, 3.125)]\n",
    "\n",
    "# drop rows with headway greater than 62m i.e. only consider below threshold\n",
    "filtered_accn_df = filtered_accn_df[filtered_accn_df['headway'] <= 124] #62 is 2s rule, 124 is 4s rule # 124\n",
    "\n",
    "# size of data\n",
    "print(f\"Shape of the new data: {filtered_accn_df.shape}\")\n",
    "\n",
    "# plot again \n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax[0].hist(filtered_accn_df['velocity'], bins=250, color='blue')\n",
    "ax[0].set_xlabel('Velocity (m/s)')\n",
    "ax[0].set_ylabel('Frequency')\n",
    "ax[0].set_title('Velocity Distribution')\n",
    "\n",
    "ax[1].hist(filtered_accn_df['headway'], bins=250, color='green')\n",
    "ax[1].set_xlabel('Headway (m)')\n",
    "ax[1].set_ylabel('Frequency')\n",
    "ax[1].set_title('Headway Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcbc444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only consider a vehicle if the total consecutive time (without missing data) is greater than 10 seconds\n",
    "# group by leader ids (imporaant, the same leader has to stick around for the threshold time)\n",
    "groups = filtered_accn_df.groupby(['leader_id'])\n",
    "print(\"Found\", len(groups), \"groups.\")\n",
    "\n",
    "# for each group sort by timesteps and check for consecutive timesteps\n",
    "# if consecutive timesteps is greater than 5 seconds then keep the group else remove the group\n",
    "# if the group is kept then append to a new dataframe\n",
    "new_df = pd.DataFrame(columns=['id', 't', 'x', 'direction', 'lane_id', 'leader_id', 'headway', 'velocity', 'acceleration'])\n",
    "\n",
    "for group_idx, (group_name, group) in enumerate(groups, start=1):\n",
    "    \n",
    "    group = group.sort_values(['t'])  \n",
    "    group['delta_t'] = group.groupby('id')['t'].diff()\n",
    "    \n",
    "    # the first one is going to have a nan\n",
    "    group['delta_t'].replace(0, np.nan, inplace=True)\n",
    "    group = group.dropna()\n",
    "\n",
    "    #print(group['delta_t'])\n",
    "    if group['delta_t'].sum() >= 5:\n",
    "        new_df = new_df.append(group, sort=False)\n",
    "\n",
    "# drop delta_t\n",
    "new_df = new_df.drop(['delta_t'], axis=1)\n",
    "\n",
    "# save to a file\n",
    "print(f\"Shape of the data: {new_df.shape}\")\n",
    "new_df.to_csv('./Data/i_24/final_i24.csv', header=True, index=False)\n",
    "\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "new_df.head(20)\n",
    "\n",
    "# Our setup allows ego vehicle to maintain a small headway at speed greater > 10% of the speed limit\n",
    "# 172294"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04147c7e",
   "metadata": {},
   "source": [
    "### Step 6: More analytics for (intensity, frequency, duration) + Perform Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cc2879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed data (which is small enough to be uploaded to GitHub as well)\n",
    "data_real = pd.read_csv('./Data/i_24/final_i24.csv') \n",
    "\n",
    "# Intensity\n",
    "# plot histogram of acceleration\n",
    "fig, ax = plt.subplots(figsize=(8, 8), dpi=100)  \n",
    "ax.hist(data_real['acceleration'], bins=5000, color='skyblue', edgecolor='black')\n",
    "ax.set_xlim(-6, 6)\n",
    "ax.set_xlabel('Acceleration (m/s^2)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Acceleration Histogram')\n",
    "ax.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Length/ Duration of accelerations\n",
    "# plot histogram of acceleration durations in a specific bucket\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Frequency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e875c757",
   "metadata": {},
   "source": [
    "### Step 7: Final Plot for paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583464dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './Data/Typical_IDM_data/'\n",
    "files = os.listdir(directory)\n",
    "file_directories = [] \n",
    "for i in files:\n",
    "    file_directories.append(directory + i)\n",
    "file_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a491cb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accelerations should not be in free flow, neither should they be in SGT\n",
    "# get all idm vehicles acceleration data before shock time\n",
    "def get_realized_accel(data, start_time, end_time):\n",
    "    # get all unique vehicle ids\n",
    "    vehicle_ids = data['id'].unique()\n",
    "\n",
    "    # get all idm vehicles, if the id has human \n",
    "    idm_vehicles = [id for id in vehicle_ids if 'idm' in id]\n",
    "\n",
    "    all_vehicle_data = []\n",
    "    # get all idm vehicles acceleration data before shock time\n",
    "    for veh in idm_vehicles:\n",
    "        veh_data = data[data['id'] == veh]\n",
    "        # Include both start and end time\n",
    "        veh_data = veh_data[(veh_data['time'] >= start_time) & (veh_data['time'] <= end_time)]\n",
    "        #veh_data = veh_data[veh_data['time'] > start_time]\n",
    "        # just get realized_accel \n",
    "        veh_data = veh_data['target_accel_with_noise_no_failsafe']\n",
    "        all_vehicle_data.append(veh_data)\n",
    "    return all_vehicle_data\n",
    "\n",
    "\n",
    "\n",
    "# For a certain initial period the system is mostly stable and near zero accelerations are present\n",
    "start_time = 80\n",
    "end_time = 160 #220\n",
    "\n",
    "all_data_idm = []\n",
    "for j in file_directories:\n",
    "    data = get_realized_accel(pd.read_csv(j), start_time, end_time)\n",
    "    for k in data:\n",
    "        all_data_idm.extend(k)\n",
    "print(len(all_data_idm)) # The total volume of data has to be the same (to just look at thier distributions)\\\n",
    "\n",
    "# randomly sample (with equal probability) the same volume \n",
    "all_data_idm_sampled = all_data_idm#random.sample(all_data_idm, len(all_data))\n",
    "\n",
    "#print(len(all_data))\n",
    "print(len(all_data_idm_sampled))\n",
    "\n",
    "plt.hist(all_data_idm_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60f9756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "plt.rcParams[\"text.color\"] = 'black'\n",
    "sns.set_style(\"whitegrid\", {\"grid.linestyle\": \"--\"})\n",
    "fontsize = 14\n",
    "\n",
    "plt.subplots_adjust( right=0.5)\n",
    "fig, ax = plt.subplots(figsize=(5, 3), dpi=300)\n",
    "fig.subplots_adjust(right=2.85)  # Add padding of 15% of the plot width to the right\n",
    "\n",
    "plt.yticks(fontsize=fontsize)\n",
    "plt.xticks(fontsize=fontsize)\n",
    "\n",
    "ax.set_xlim(-3.1, 3.1)\n",
    "ax.set_ylim(0, 41e3)\n",
    "ax.set_yticks( [10000, 20000, 30000, 40000] )#np.linspace(0.0, 6.5e3, 5))\n",
    "ax.set_xticks( [ -3, -2, -1, 0, 1, 2, 3] )\n",
    "\n",
    "n_bins = 1000 # Number of equal width bins in the range\n",
    "n_bins_2 = np.linspace(-4, 4, 120)\n",
    "\n",
    "# ax.hist(data_real['acceleration'], bins=n_bins, histtype='bar', facecolor = '#2ab0ff', alpha =1.0, edgecolor='#169acf', linewidth=1.0, label = \"Real-world\")# ,histtype=\"stepfilled\", zorder=1)\n",
    "# ax.hist(all_data_idm_sampled, bins= n_bins, histtype='bar', facecolor = '#FFA500', alpha =0.6, edgecolor='#FA8128', linewidth=1.0, label = \"Simulated\") #, histtype=\"stepfilled\", zorder=2)\n",
    "\n",
    "ax.hist(data_real['acceleration'], bins=n_bins_2, facecolor = '#2ab0ff', alpha =1.0, edgecolor='#169acf', linewidth=0.8, label = \"Real-world Traffic\", histtype=\"bar\", range=(-3, 3))\n",
    "ax.hist(all_data_idm_sampled, bins= n_bins_2, facecolor = '#FFA500', alpha =0.7, edgecolor='#FA8128', linewidth=0.8, label = \"Simulated Traffic\", histtype=\"bar\", range=(-3, 3))\n",
    "\n",
    "\n",
    "formatter = mticker.ScalarFormatter(useMathText=True)\n",
    "formatter.set_powerlimits((-3, 3))\n",
    "formatter.set_scientific(True)\n",
    "ax.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "fs = 13\n",
    "ax.set_xlabel(\"Instantaneous Acceleration $(m/s^{2})$\", fontsize = fs)\n",
    "ax.set_ylabel(\"Frequency\", fontsize = fs)\n",
    "\n",
    "\n",
    "ax.legend(fontsize = fs-2, loc='upper right')                             \n",
    "fig.tight_layout()\n",
    "plt.savefig('./real_accel.pdf', format ='pdf')\n",
    "plt.savefig('./real_accel_png.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8450fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage inside and outside [-1, 1] in all_data_idm_sampled and data_real['acceleration']\n",
    "# Percentage of accelerations inside [-1, 1] in all_data_idm_sampled\n",
    "inside = 0\n",
    "outside = 0\n",
    "for i in all_data_idm_sampled:\n",
    "    if i >= -0.5 and i <= 0.5:\n",
    "        inside += 1\n",
    "    else:\n",
    "        outside += 1\n",
    "print(f\"Percentage of accelerations inside [-1, 1] in all_data_idm_sampled: {inside/len(all_data_idm_sampled)}\")\n",
    "\n",
    "# Percentage of accelerations inside [-1, 1] in data_real['acceleration']\n",
    "inside = 0\n",
    "outside = 0\n",
    "for i in data_real['acceleration']:\n",
    "    if i >= -0.5 and i <= 0.5:\n",
    "        inside += 1\n",
    "    else:\n",
    "        outside += 1\n",
    "print(f\"Percentage of accelerations inside [-1, 1] in data_real['acceleration']: {inside/len(data_real['acceleration'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aca2717",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
